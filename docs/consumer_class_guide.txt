generator_and_consumer/consumer_class.py – guida operativa

Indice sintetico
1. Scopo e panoramica
2. Configurazione e attributi principali
3. Ciclo di vita del consumer
4. Accesso ai dati
5. Utilizzi tipici
6. Estensioni e personalizzazioni
7. Suggerimenti pratici

1. Scopo e panoramica
- La classe `KafkaConsumer` incapsula la logica per ricevere misure da un topic Kafka in tempo reale.
- È pensata per scenari “online”: avviata in un thread dedicato, mantiene un buffer circolare degli ultimi N campioni e mette a disposizione funzioni di utilità per l’orchestratore EMS.

2. Configurazione e attributi principali
- Parametri del costruttore:
  * `buffer_size` (default 96): dimensione massima dei buffer circolari (≈ 24 h a step da 15 min).
  * `topic`: topic Kafka da sottoscrivere.
  * `timezone` (default `Europe/Rome`): fuso usato per normalizzare i timestamp dei messaggi.
- Attributi aggiornati in tempo reale:
  * `timestamps`, `solar`, `load`: `collections.deque` con `maxlen=buffer_size`.
  * `total_messages`: contatore dei messaggi validi ricevuti.
  * `consumer`, `running`, `thread`: stato del client Kafka e del thread di polling.

3. Ciclo di vita del consumer
- `connect()`: recupera l’endpoint Kafka tramite l’API gateway (`/register/dc`), crea il consumer e si sottoscrive al topic.
- `start_background()`: invoca `connect()`, imposta `running=True` e avvia `_loop()` in un thread separato (non daemon, così è possibile bloccare l’uscita in maniera ordinata).
- `_loop()`:
  * esegue `poll(1.0)` finché `running` è `True`;
  * scarta messaggi nulli o con errore;
  * decodifica il payload JSON, converte il timestamp in `datetime` (tz-aware, riportato nel fuso configurato);
  * preleva i valori `solar`/`load` (forzati a float ≥ 0) e li accoda nelle relative deques;
  * incrementa `total_messages`.
- `stop()`: ferma l’esecuzione con sicurezza (`running=False`, `thread.join(timeout=3)`, chiusura del consumer).

4. Accesso ai dati
- **Ultimo campione**: tipicamente si usa una funzione helper come `deque_last(consumer.load, default)` per lavorare sul valore più fresco (pattern adottato da `ems_realtime_kafka.py`).
- **Snapshot dell’intero buffer**: `get_data()` restituisce un `pandas.DataFrame` con le colonne `datetime`, `solar`, `load` valorizzate con tutti i campioni conservati.
- **Controllo riempimento**: `is_ready()` indica se il buffer ha già raggiunto la capienza (`len(self.solar) >= buffer_size`), utile durante la fase di bootstrap.

5. Utilizzi tipici
1. **EMS in streaming**: avvio in background, lettura dell’ultimo campione a ogni step e alimentazione di un controllore o simulatore in tempo reale.
2. **Record & replay**: accumulo per un certo periodo, salvataggio del DataFrame (`consumer.get_data().to_csv(...)`) e rianalisi offline.
3. **Analisi rolling**: calcolo di medie/massimi mobili usando porzioni della deque (`np.mean(list(consumer.load)[-4:])`).
4. **Dashboard live**: schedulazione periodica di letture `get_data()` per visualizzazioni aggiornate (Streamlit, Plotly Dash, ecc.).
5. **Monitoraggio qualità dati**: inserimento di logiche di alert in `_loop()` o in un task secondario (es. se `time.time() - timestamps[-1]` supera una soglia).

6. Estensioni e personalizzazioni
- **Logging errori**: sostituire il blocco `except: continue` con logging strutturato o metriche.
- **Topic multipli**: instanziare più consumer o estendere la classe per gestire mapping topic→buffer.
- **Persistenza su disco**: integrare la scrittura su file/DB in `_loop()` per avere uno storico completo.
- **Buffer adattivo**: esporre metodi per cambiare `maxlen` a runtime o per sviluppare statistiche su finestre diverse.
- **Gestione heartbeat**: aggiungere callback o metriche Prometheus per monitorare il polling.

7. Suggerimenti pratici
- Controllare `consumer.total_messages` per sincronizzare il loop principale ed evitare letture duplicate (come fa `ems_realtime_kafka.py` con `last_count`).
- Ricordare che la deque scarta automaticamente i campioni più vecchi: se serve mantenere più giorni di dati, aumentare `buffer_size`.
- Impostare `timezone` coerente con le misure ricevute per evitare discrepanze nelle fasce orarie.
- Per test o debug rapido, usare `consumer.get_data().tail()` e `consumer.stop()` a fine esecuzione per rilasciare le risorse Kafka.
